# -*- coding: utf-8 -*-
"""To run and test the model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zQkP6yk76jWtMwNwyg7RMIrrn0je6_JV

**WHAT WE'RE TRAINING : A TENSORFLOW-KERAS NN \
 INPUT DATA :COMPLEX PROB AMPS OF A 2 PHOTON STATE (8X1) (4 complex numbers with real and imag parts next to each other)\
 OUTPUT DATA: MEASUREMENT MATRICES (36X1)**

Upload the attached files to your google drive.

and run the code below:

this model doesnt predict perfectly well right now...I will improve the training.
"""

from google.colab import drive
drive.mount('/content/drive')

#model load methods

import os
os.chdir('/content/drive/My Drive/project stuuf/1 full span utter nonsense')#model-limited span v1') #model files-upload to drive-need to retrain') #

from tensorflow.keras.models import model_from_json, load_model

# Load the model architecture from the JSON file.
with open('model.json', 'r') as json_file:
    loaded_model_json = json_file.read()
    loaded_model = model_from_json(loaded_model_json)

# Load the model weights.
loaded_model.load_weights('model_weights.h5')

import os
import numpy as np

# Define the path to the folder within your Google Drive
folder_path = '/content/drive/MyDrive/project stuuf' #/model final for 23 october submission'

# Change the working directory to the folder_path
os.chdir(folder_path)

# List the content of the folder
files = os.listdir(folder_path)
print(files)

# Load the NumPy array from the .npy file
outputstotest = np.load('outputstotest.npy')
inputstotest =np.load('inputstotest.npy')

#print(np.shape(outputstotest))
#print(np.size(inputstotest[0]))

predictions=loaded_model.predict(inputstotest)
print(predictions)
print(predictions)

predDM=[]
# complex density matrix
for prediction in predictions:
  Psi= [prediction[0]+prediction[1]*1j, prediction[2]+prediction[3]*1j, prediction[4]+prediction[5]*1j, prediction[6]+prediction[7]*1j]
  density_matrix=np.outer(Psi, np.conj(Psi))
  predDM.append(density_matrix)
predDM=np.array(predDM).reshape(5, 16)
print(predDM)

predDMreal=np.array(predDM.real)
predDMimag=np.array(predDM.imag)
  #print(real, imag) #good for plotting!
print(predDMreal)
print(np.shape(predDMreal))

print(np.shape(outputstotest))
print(outputstotest[4])
outputstotest=np.array(outputstotest.real)

#ERROR PLOT
num=5
predDM=predDM.reshape(16*5)
outputstotest

errors=[]
for j, k in zip(predDM,outputstotest):
    error= (predDM[i],outputstotest[i])/ *100
    errors.append(error)
errors=np.array(errors)
print(errors)
print(np.shape(errors))
average_error= sum(errors)/1600
print("average error=", average_error, "%")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(num), errors, label='dot(testers, predictions)', marker='o')
plt.xlabel('datapoint')
plt.ylabel('error (%)')
plt.title('dot product average error = , (for *** dp noisy data)')
plt.legend()
plt.grid(True)
plt.show()

#ERROR PLOT

testers=np.reshape(prob_amps1,(50,8))
predictions=loaded_model.predict(Meas1)
predictions=np.array(predictions)

j=0
errordot=[]
for i in range(num):
  error=np.dot(predictions[j], testers[j])
  errordot.append(error)
  j=j+1
errors=np.array(errordot)-np.ones(num)
print(errors)

s=0
for e in errors:
  s += e**2
print(s) #works!

toplot=(errors/1)*100

average_error= sum((errors/1)*100)/num
print("average error=", average_error, "%")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(num), toplot, label='dot(testers, predictions)', marker='o')
plt.xlabel('datapoint')
plt.ylabel('error (%)')
plt.title('dot product average error = , (for *** dp noisy data)')
plt.legend()
plt.grid(True)
plt.show()

#prerequisites important to run
import numpy as np
import pandas as pd
import math

#the 6 basis states
R = np.array([[1],[0]], ndmin=2)
L = np.array([[0],[1]], ndmin=2)
H = 1/math.sqrt(2)*np.array([[1],[1j]], ndmin=2)
V = -1j*1/math.sqrt(2)*np.array([[1],[-1j]], ndmin=2)
D = 1/math.sqrt(2)*np.array([[1],[1]], ndmin=2)
A = 1j*1/math.sqrt(2)*np.array([[1],[-1]], ndmin=2)

list_1= [R,L,H,V,D,A]

#projectors for 2 particles
empty2=[]
for i in list_1:
  empty1=[]
  for j in list_1:
    m= np.kron(i,j) #tensor product of the 2 photons polarizations
    proj=np.outer(m, np.conj(m))
    pro=proj.flatten()
    empty2.append(pro)
projectors=np.array(empty2)

#A TEST OF THE MODEL ON LAB DATA (THE ONE NEELAN SENT ME) - USE DENSITY MATRIX PLOTS BELOW TO CHECK THIS...

array=np.array([[3148,97244,38225,53760, 44976,	31086],[101615,	4278,43668,27201,39972,	56495],
[36851,	43986,	72903,	26977,	2289,33031],
[54951,29463,	31561,78925	,26869,4530],
[45032,	40293,2178,	27971,81871, 28615],
[31010,	55528,27324,4376,	32577, 76932]]).reshape(1,36)
array1= array/(101615+97244)
lab_test=loaded_model.predict(array1)

#to convert predicted state to a 4x1:
prediction=np.array([(lab_test[0][0]+lab_test[0][1]*1j), (lab_test[0][2]+lab_test[0][3]*1j), (lab_test[0][4]+lab_test[0][5]*1j), (lab_test[0][6]+lab_test[0][7]*1j)])
print("the predicted state:", (prediction*4)-1)

#a check - poor score out of 1 could be because the test data is very noisy.
print("the sum of the squares of prediction=", ((lab_test[0][0] + lab_test[0][1]*1j)*(lab_test[0][0] - lab_test[0][1]*1j) + (lab_test[0][2]+lab_test[0][3]*1j)*(lab_test[0][2]-lab_test[0][3]*1j) + (lab_test[0][4]+lab_test[0][5]*1j)*(lab_test[0][4]-lab_test[0][5]*1j) +(lab_test[0][6]+lab_test[0][7]*1j)*(lab_test[0][6]-lab_test[0][7]*1j)))

import seaborn as sns
import matplotlib.pyplot as plt

states=['R','L','H','V','D','A']

cmapreal=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(array1.reshape(6,6), vmin=0, vmax=1, cmap=cmapreal, xticklabels=states, yticklabels=states)
hmreal.set_xlabel(' photon A state', fontsize=10)
hmreal.set_ylabel('photon B state', fontsize=10)
#plt.title('Input - Lab Measurement Matrix') #generated and predicted display next to each other

#PLOT - PREDICTION VS LAB TEST

import seaborn as sns
import matplotlib.pyplot as plt

labels=['Al','B','C','D']
# complex density matrix
Psi= prediction
density_matrix=np.outer(Psi, np.conj(Psi))
#to seperate into real and imag:
real=density_matrix.real
imag=density_matrix.imag
#print(real, imag) #good for plotting!

fig, axs = plt.subplots(ncols=2)
cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(real, vmin=0, vmax=1, center=0.5, ax=axs[0], cmap=cmap, xticklabels=labels, yticklabels=labels)

cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmim=sns.heatmap(imag, vmin=0, vmax=1, center=0.5, ax=axs[1], cmap=cmap, xticklabels=labels, yticklabels=labels)
plt.title('Predicted State - (a) Real and (Imaginary Parts of Density Matrix')

#PLOTS - EXPECTED STATE 0 1/sqrt(2) 1/sqrt(2) 0

# complex density matrix
Psi1= [0+0*1j ,(1/math.sqrt(2))+ 0*1j , (1/math.sqrt(2))+ 0*1j, 0+0*1j ]
density_matrix1=np.outer(Psi1, np.conj(Psi1))
#to seperate into real and imag:
real1=density_matrix1.real
imag1=density_matrix1.imag
#print(real, imag) #good for plotting!

fig1, axs1 = plt.subplots(ncols=2)
cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(real1, vmin=0, vmax=1, center=0.5, ax=axs1[0], cmap=cmap, xticklabels=labels, yticklabels=labels)

cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmim=sns.heatmap(imag1, vmin=0, vmax=1, center=0.5, ax=axs1[1], cmap=cmap, xticklabels=labels, yticklabels=labels)
plt.title('THEORETICAL STATE-real and imaginary parts of density matrix')

#CONFIRMATION BY MUSTAFAS ANALYSIS
r=np.array([0.015,0.073,0.074,-0.011,	0.073,	0.41,	0.46,	-0.062,	0.074,	0.46,	0.56,	-0.064,	-0.011,	-0.062,	-0.064,	0.011]).reshape(4,4)
i=np.array([0, 0.0082,	0.012,	-0.0051,-0.0082,	0,	0.032,	-0.021,-0.012,	-0.032,	0,	-0.019, 0.0051,	0.021,	0.019,	0]).reshape(4,4)

fig2, axs2 = plt.subplots(ncols=2)
xmin, xmax = plt. xlim()
plt. xlim(xmin * 2, xmax * 2)
cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(r, vmin=0, vmax=1, center=0.5, ax=axs2[0], cmap=cmap, xticklabels=labels, yticklabels=labels)

cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmim=sns.heatmap(i, vmin=0, vmax=1, center=0.5, ax=axs2[1], cmap=cmap, xticklabels=labels, yticklabels=labels)

#A TEST ON UNIQUELY SIMULATED/GENERATED DATA

num=100000

#Complex prob amps
lst=[]
tst=[]                                                              #Create a list to fill with generated prob amplitudes
for i in range(num):
  u=[]
  for i in range(4):
    real=np.random.uniform(0, 1)
    comp=np.random.uniform(0, 1)*1j
    u.append(real)
    u.append(comp)
  values=np.array([(u[0]+u[1]), (u[2]+u[3]), (u[4]+u[5]), (u[6]+u[7])])
  div=np.sum(values)
  norm=values/div
  u1=np.sqrt(norm)
  tst.append(u1)
  u2=np.array([u1[0].real, u1[0].imag, u1[1].real, u1[1].imag,  u1[2].real, u1[2].imag, u1[3].real, u1[3].imag]) #seperating into 8 seperate values but this gets rid of the imag number - j !!!
  lst.append(u2)
fourvec = np.array(tst) #this is the prob amps as a 4x1
prob_amps1=np.array(lst) #this is the prob amps as an 8x1


#to check that the sum of the squares of the prob amps are 1.
s=0
for e in fourvec[0]:
  s += e**2
print("to check that the sum of the squares of the prob amps for gen data are 1:",s) #works!    # could use math.isclose(); this function aims to allow you to ‘ignore’ the small differences between floating point values, the imaginary part here should be 0


# complex density matrix
density_matrices=[]
for i in range(num):
  Psi=fourvec[i]
  p_12=np.outer(Psi, np.conj(Psi))
  p12=p_12.flatten()
  density_matrices.append(p12)

#measurement=expectation value matrix/dataframe , will need to get 36 measurements for each of the 20 states - array of 20 arrays that contain 6x6 matrices (36 elements)
Measurements=[]
for dm in density_matrices:
  list1=[]
  for j in projectors:
    expectation_val=np.matmul(dm, j)
    expectationval_noise= (expectation_val.real)*(np.random.uniform(1,1.15))
    list1.append(expectationval_noise)
  Measurements.append(list1)
Meas1=np.array(Measurements)

#ERROR PLOT

testers=np.reshape(prob_amps1,(50,8))
predictions=loaded_model.predict(Meas1)
predictions=np.array(predictions)

j=0
errordot=[]
for i in range(num):
  error=np.dot(predictions[j], testers[j])
  errordot.append(error)
  j=j+1
errors=np.array(errordot)-np.ones(num)
print(errors)

s=0
for e in errors:
  s += e**2
print(s) #works!

toplot=(errors/1)*100

average_error= sum((errors/1)*100)/num
print("average error=", average_error, "%")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(num), toplot, label='dot(testers, predictions)', marker='o')
plt.xlabel('datapoint')
plt.ylabel('error (%)')
plt.title('dot product average error = , (for *** dp noisy data)')
plt.legend()
plt.grid(True)
plt.show()

import math
#A TEST ON A KNOWN STATE - which is GENERATED/SIMULATED
Al=1/math.sqrt(2)
Aimag=0
B=0
Bimag=0
C=0
Cimag=0
D=-1j/math.sqrt(2)
Dimag=0

#density matrix
state= np.array([[Al], [Aimag], [B], [Bimag],[C], [Cimag],[D], [Dimag]])
Psi= [Al + Aimag*1j,B + Bimag*1j, C + Cimag*1j,D + Dimag*1j]

dm1=np.outer(Psi, np.conj(Psi))
dm=dm1.flatten()

list1=[]
for i in projectors:
  expectation_val=np.matmul(dm, i)
  list1.append(expectation_val)
M=np.array(list1).real

import seaborn as sns
import matplotlib.pyplot as plt

states=['R','L','H','V','D','A']

cmapreal=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(M.reshape(6,6), annot=True, cmap=cmapreal, xticklabels=states, yticklabels=states)
hmreal.set_xlabel(' photon 1 state', fontsize=10)
hmreal.set_ylabel('photon 2 state', fontsize=10)
plt.title('measurement matrix input') #generated and predicted display next to each other

test = loaded_model.predict(M.reshape(1,36))


#PLOTS - PREDICTION

import seaborn as sns
import matplotlib.pyplot as plt

labels=['Al','B','C','D']

#to convert predicted state to a 4x1:
prediction2=np.array([(test[0][0]+test[0][1]*1j), (test[0][2]+test[0][3]*1j), (test[0][4]+test[0][5]*1j), (test[0][6]+test[0][7]*1j)])
density_matrix=np.outer(prediction2, np.conj(prediction2))
print("the predicted state:", prediction2)

#a check - poor score out of 1 could be because the test data is very noisy.
print("the sum of the squares predicted=", ((test[0][0] + test[0][1]*1j)*(test[0][0] - test[0][1]*1j) + (test[0][2]+test[0][3]*1j)*(test[0][2]-test[0][3]*1j) + (test[0][4]+test[0][5]*1j)*(test[0][4]-test[0][5]*1j) +(test[0][6]+test[0][7]*1j)*(test[0][6]-test[0][7]*1j)))

#to seperate into real and imag:
real=density_matrix.real
imag=density_matrix.imag
fig, axs = plt.subplots(ncols=2)
cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(real, vmin=0, vmax=1, center=0.5, ax=axs[0], annot=True, cmap=cmap, xticklabels=labels, yticklabels=labels)

cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmim=sns.heatmap(imag, vmin=0, vmax=1, center=0.5, ax=axs[1], annot=True, cmap=cmap, xticklabels=labels, yticklabels=labels)
plt.title('PREDICTED STATE OUTPUT-real and imaginary parts of density matrix')

#PLOT - EXPECTED

#to seperate into real and imag:
real1=dm1.real
imag1=dm1.imag

fig, axs1 = plt.subplots(ncols=2)
cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmreal=sns.heatmap(real1, vmin=0, vmax=1, center=0.5, ax=axs1[0], annot=True, cmap=cmap, xticklabels=labels, yticklabels=labels)

cmap=sns.cubehelix_palette(n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, light=0.85, dark=0.15, reverse=False, as_cmap=True)
hmim=sns.heatmap(imag1, vmin=0, vmax=1, center=0.5, ax=axs1[1], annot=True, cmap=cmap, xticklabels=labels, yticklabels=labels)
plt.title('THEORETICAL STATE-real and imaginary parts of density matrix')

Al=1/math.sqrt(2)
Aimag=0
B=0
Bimag=0
C=0
Cimag=0
D=1/math.sqrt(2)
Dimag=0

#density matrix
state= np.array([[Al], [Aimag], [B], [Bimag],[C], [Cimag],[D], [Dimag]])
Psi= [Al + Aimag*1j,B + Bimag*1j, C + Cimag*1j,D + Dimag*1j]

dm1=np.outer(Psi, np.conj(Psi))

"""**NO NEED TO RUN THE FOLLOWING, THIS IS THE DATA AND MODEL CODE FOR BUG/CORRECTION CHECKING THE MODEL AND TRAINING DATA USED:**"""

#GENERATION OF COMPLEX DATA FOR TRAINING

#the 6 basis states
R = np.array([[1],[0]], ndmin=2)
L = np.array([[0],[1]], ndmin=2)
H = 1/math.sqrt(2)*np.array([[1],[1j]], ndmin=2)
V = -1j*1/math.sqrt(2)*np.array([[1],[-1j]], ndmin=2)
D = 1/math.sqrt(2)*np.array([[1],[1]], ndmin=2)
A = 1j*1/math.sqrt(2)*np.array([[1],[-1]], ndmin=2)

list_1= [R,L,H,V,D,A]

#projectors for 2 particles
empty2=[]
for i in list_1:
  empty1=[]
  for j in list_1:
    m= np.kron(i,j) #tensor product of the 2 photons polarizations
    proj=np.outer(m, np.conj(m))
    pro=proj.flatten()
    empty2.append(pro)
projectors=np.array(empty2)

dp= 1050000  #dp are the number of datapoints (the number of unique sets of {Al, B, C and D})

#Complex prob amps
lst=[]
tst=[]                                                              #Create a list to fill with generated prob amplitudes
for i in range(dp):
  u=[]
  for i in range(4):
    real=np.random.uniform(0, 1)
    comp=np.random.uniform(0, 1)*1j
    u.append(real)
    u.append(comp)
  values=np.array([(u[0]+u[1]), (u[2]+u[3]), (u[4]+u[5]), (u[6]+u[7])])
  div=np.sum(values)
  norm=values/div
  u1=np.sqrt(norm)
  tst.append(u1)
  u2=np.array([u1[0].real, u1[0].imag, u1[1].real, u1[1].imag,  u1[2].real, u1[2].imag, u1[3].real, u1[3].imag]) #seperating into 8 seperate values but this gets rid of the imag number - j !!!
  lst.append(u2)
fourvec = np.array(tst) #this is the prob amps as a 4x1
prob_amps1=np.array(lst) #this is the prob amps as an 8x1
print("fourvec=", fourvec[0])


#to check that the sum of the squares of the prob amps are 1.
s=0
for e in fourvec[0]:
  s += e**2
print(s) #works!    # could use math.isclose(); this function aims to allow you to ‘ignore’ the small differences between floating point values, the imaginary part here should be 0


# complex density matrix
density_matrices=[]
for i in range(dp):
  Psi=fourvec[i]
  p_12=np.outer(Psi, np.conj(Psi))
  p12=p_12.flatten()
  print("shape of projector=", np.shape(p12[0]))
  density_matrices.append(p12)
print("dm=", density_matrices[0])

#to seperate into real and imag: #good for plotting!
real=density_matrices[0].real
imag=density_matrices[0].imag
#print(real, imag)

#measurement=expectation value matrix
Measurements=[]
for dm in density_matrices:
  list1=[]
  for j in projectors:
    expectation_val=np.matmul(dm, j)
    expectationval_noise= (expectation_val.real)*(np.random.uniform(1,1.15))
    list1.append(expectationval_noise)
  Measurements.append(list1)
Meas1=np.array(Measurements)
#print("Meas=",Meas1[0], np.shape(Meas1))

#FOR TRAINING THE DATA

#tensorflow method
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

dp= 1050000
input_shape = (36, 1)
output_shape = 8

# Define the dataset (input matrices and corresponding output lists)
input_data = Meas1
output_data = prob_amps1

# Create a custom callback to store epoch history
class CustomHistoryCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if 'epoch_history' not in vars(self):
            self.epoch_history = []

        self.epoch_history.append({
            'epoch': epoch + 1,
            'loss': logs['loss'],
            'val_loss': logs['val_loss']
            # Add other metrics as needed
        })

# Split the dataset into training and testing sets
split_ratio = 0.7
split_index = int(dp * split_ratio)

train_input = input_data[:split_index]
train_output = output_data[:split_index]
test_input = input_data[split_index:]
test_output = output_data[split_index:]

# Define a neural network model
model = keras.Sequential([
    layers.Input(shape=input_shape),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(8)  # Output layer with 4 neurons for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse']) #adagrad adadelta


# Train the model
batch_size = 30 #dec
epochs = 20 #inc


# Create an instance of the custom callback
custom_callback = CustomHistoryCallback()

history=model.fit(train_input, train_output, batch_size=batch_size, epochs=epochs, validation_data=(test_input, test_output), callbacks=[custom_callback])

# Access the epoch history as a list of dictionaries
epoch_history = custom_callback.epoch_history

# Convert the epoch history to a NumPy array
epoch_history_array = np.array(epoch_history)

# Evaluate the model on test data
test_loss, test_mse = model.evaluate(test_input, test_output)
print(f"Test Loss: {test_loss}, Test MSE: {test_mse}")